{"cells":[{"cell_type":"code","execution_count":null,"id":"df2a0911-bb23-447d-9139-fdefb13c912d","metadata":{"id":"df2a0911-bb23-447d-9139-fdefb13c912d"},"outputs":[],"source":["# Load Data\n","import numpy as np\n","import scipy.io as sio\n","datas = sio.loadmat('dataset_mapping.mat')\n","bed_select=datas['bed_select']\n","bed_whole=datas['bed_whole']\n","pillow_up=datas['pillow_up']\n","person = datas['person']\n","posture = datas['posture']"]},{"cell_type":"code","execution_count":null,"id":"a17137f8-3ffb-43dc-8bef-e651f878c705","metadata":{"id":"a17137f8-3ffb-43dc-8bef-e651f878c705"},"outputs":[],"source":["i=15\n","ood_idx = np.where(posture==i)[0]\n","indis_idx = np.setdiff1d(np.arange(len(posture)), ood_idx)\n","indis_input = bed_select[indis_idx,:,:]\n","ood_input = bed_select[ood_idx,:,:]\n","indis_output = pillow_up[indis_idx,:,:]\n","ood_output = pillow_up[ood_idx,:,:]\n","posture = tf.keras.utils.to_categorical(posture)\n","posture_indis = posture[indis_idx,:]\n","posture_ood = posture[ood_idx,:]\n","X_tr, X_te, y_tr, y_te,posture_tr, posture_te = train_test_split(indis_input, indis_output,posture_indis, train_size=0.9)\n","scaler = MinMaxScaler()\n","scaler2 = MinMaxScaler()\n","reshaped_xtr = X_tr.reshape(X_tr.shape[0],-1)\n","reshaped_xte = X_te.reshape(X_te.shape[0],-1)\n","reshaped_ytr = y_tr.reshape(y_tr.shape[0],-1)\n","reshaped_yte = y_te.reshape(y_te.shape[0],-1)\n","reshaped_xood = ood_input.reshape(ood_input.shape[0],-1)\n","reshaped_yood = ood_output.reshape(ood_output.shape[0],-1)\n","\n","normalized_xtr1 = scaler.fit_transform(reshaped_xtr)\n","normalized_xte1 = scaler.transform(reshaped_xte)\n","normalized_xood1 = scaler.transform(reshaped_xood)\n","normalized_ytr1 = scaler2.fit_transform(reshaped_ytr)\n","normalized_yte1 = scaler2.transform(reshaped_yte)\n","normalized_yood1 = scaler2.transform(reshaped_yood)\n","\n","normalized_xtr = normalized_xtr1.reshape(X_tr.shape)\n","normalized_xte = normalized_xte1.reshape(X_te.shape)\n","normalized_ytr = normalized_ytr1.reshape(y_tr.shape)\n","normalized_yte = normalized_yte1.reshape(y_te.shape)\n","normalized_xood = normalized_xood1.reshape(ood_input.shape)\n","normalized_yood = normalized_yood1.reshape(ood_output.shape)\n","input_shape = (16,40,1)\n","output_shape = (32,48,1)\n","normalized_xtr = normalized_xtr[:,np.newaxis,:,:]\n","normalized_xte = normalized_xte[:,np.newaxis,:,:]\n","normalized_ytr = normalized_ytr[:,np.newaxis,:,:]\n","normalized_yte = normalized_yte[:,np.newaxis,:,:]\n","normalized_xood = normalized_xood[:,np.newaxis,:,:]\n","normalized_yood = normalized_yood[:,np.newaxis,:,:]\n","\n","posture_tr = np.squeeze(posture_tr)\n","posture_te = np.squeeze(posture_te)"]},{"cell_type":"code","execution_count":null,"id":"2e5c5839-0858-4abf-9a8c-28f40dbf69d7","metadata":{"id":"2e5c5839-0858-4abf-9a8c-28f40dbf69d7"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Function\n","class DANNModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(DANNModel, self).__init__()\n","        n_filters1=32\n","        hidden_size = 10\n","        output_size = (32,48,1)\n","        self.feature_extractor = nn.Sequential(\n","            nn.Conv2d(1,n_filters1,(4,6), stride=(1,1)),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(n_filters1,n_filters1*2,(3,5), stride=(1,1)),\n","            nn.ReLU(0.2),\n","            nn.Conv2d(n_filters1*2,n_filters1*2,(3,5), stride=(1,1)),\n","            nn.ReLU(0.2),\n","            nn.Conv2d(n_filters1*2,n_filters1*2,(2,4), stride=(1,1)),\n","            nn.ReLU(0.2),\n","            nn.Conv2d(n_filters1*2,n_filters1*2,(2,4), stride=(1,1)),\n","            nn.ReLU(0.2)\n","        )\n","        hidden_size = 7*21*64\n","        self.class_classifier = nn.Sequential(\n","            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(16 * 7 * 21, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, num_classes)\n","        )\n","        self.image_decoder = nn.Sequential(\n","            nn.ConvTranspose2d(n_filters1*2,n_filters1*2, (2,4), stride=(1,1)),\n","            nn.LeakyReLU(0.2),\n","            nn.ConvTranspose2d(n_filters1*2,n_filters1*2, (7,7), stride=(1,1)),\n","            nn.LeakyReLU(0.2),\n","            nn.ConvTranspose2d(n_filters1*2,n_filters1*2, (7,7), stride=(1,1)),\n","            nn.LeakyReLU(0.2),\n","            nn.ConvTranspose2d(n_filters1*2,n_filters1*2, (7,7), stride=(1,1)),\n","            nn.LeakyReLU(0.2),\n","            nn.ConvTranspose2d(n_filters1*2,1, (7,7), stride=(1,1)),\n","            nn.LeakyReLU(0.2),\n","            nn.ReLU()\n","        )\n","    def forward(self, x, alpha):\n","        features = self.feature_extractor(x)\n","        #print(features.shape)\n","        class_output = self.class_classifier(features)\n","        #print(\"Class\")\n","        #print(class_output.shape)\n","        image_output = self.image_decoder(features)\n","        return class_output, image_output"]},{"cell_type":"code","execution_count":null,"id":"e53a049a-e801-4f26-9ad8-c6db9096101d","metadata":{"id":"e53a049a-e801-4f26-9ad8-c6db9096101d"},"outputs":[],"source":["# Define the gradient reversal layer\n","class GradientReversalFunction(Function):\n","    @staticmethod\n","    def forward(ctx, x, alpha):\n","        ctx.alpha = alpha\n","        return x.view_as(x)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        output = grad_output.neg() * ctx.alpha\n","        return output, None\n","\n","def grad_reverse(x, alpha):\n","    return GradientReversalFunction.apply(x, alpha)"]},{"cell_type":"code","execution_count":null,"id":"570caa55-6f9e-41a7-bfb0-b9b634958595","metadata":{"id":"570caa55-6f9e-41a7-bfb0-b9b634958595"},"outputs":[],"source":["def train_dann(model, source_loader, target_loader, num_epochs, alpha, checkpoint_path):\n","    criterion_cls = nn.CrossEntropyLoss()\n","    criterion_img = nn.MSELoss()\n","    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","    best_loss = float('inf')\n","    for epoch in range(num_epochs):\n","        for (source_data, source_labels, source_posture), (target_data, target_posture, target_labels) in zip(source_loader, target_loader):\n","            source_data, source_posture, source_labels = source_data, source_labels, source_posture\n","\n","\n","            # Step 1: Train the feature extractor, class classifier, and image decoder on source domain\n","            optimizer.zero_grad()\n","            class_output, image_output = model(source_data, alpha)\n","            #print(class_output.dtype)\n","            #print(source_labels.dtype)\n","            loss_cls = criterion_cls(class_output, source_labels)\n","            loss_img = criterion_img(image_output, source_posture)\n","            loss_total = loss_cls + loss_img\n","            loss_total.backward()\n","\n","            # Step 2: Train the domain classifier on both source and target domain\n","            #domain_labels = torch.zeros(len(source_data) + len(target_data)).long()\n","            #domain_labels[:len(source_data)] = 1  # Source domain has label 1, target domain has label 0\n","\n","            domain_data = torch.cat((source_data, target_data), dim=0)\n","            domain_labels = torch.cat((source_labels,target_labels),dim = 0)\n","            domain_data = grad_reverse(domain_data, alpha)\n","\n","            domain_output, _ = model(domain_data, alpha)\n","            loss_domain = criterion_cls(domain_output, domain_labels)\n","            loss_domain.backward()\n","\n","            optimizer.step()\n","\n","            val_loss = loss_domain.item()\n","\n","            # Save the model checkpoint if validation loss improves\n","            if val_loss < best_loss:\n","                best_loss = val_loss\n","                torch.save(model.state_dict(), checkpoint_path)\n","                print(\"Model checkpoint saved at\", checkpoint_path)\n","\n","            print('Epoch [{}/{}]: Loss_cls: {:.4f} Loss_img: {:.4f} Val_loss_img: {:.4f}'.format(epoch+1, num_epochs, loss_cls.item(), loss_img.item(), val_loss))\n","\n","    # Load the best model checkpoint\n","    model.load_state_dict(torch.load(checkpoint_path))\n","    print(\"Best model loaded from\", checkpoint_path)"]},{"cell_type":"code","execution_count":null,"id":"79875ed7-cf42-4325-8a94-16e994004add","metadata":{"id":"79875ed7-cf42-4325-8a94-16e994004add"},"outputs":[],"source":["dataset_tr = torch.utils.data.TensorDataset(torch.tensor(normalized_xtr, dtype=torch.float32), torch.tensor(normalized_ytr, dtype=torch.float32), torch.tensor(posture_tr, dtype=torch.float32))\n","dataset_te = torch.utils.data.TensorDataset(torch.tensor(normalized_xood, dtype=torch.float32), torch.tensor(normalized_yood, dtype=torch.float32), torch.tensor(posture_ood, dtype=torch.float32))"]},{"cell_type":"code","execution_count":null,"id":"26ef1f8d-d557-4d75-96f6-6c6e9a9dbfe4","metadata":{"id":"26ef1f8d-d557-4d75-96f6-6c6e9a9dbfe4"},"outputs":[],"source":["source_loader = torch.utils.data.DataLoader(dataset_tr, batch_size=32, shuffle=True)\n","target_loader = torch.utils.data.DataLoader(dataset_te, batch_size=32, shuffle=True)\n","\n","# Initialize the model and hyperparameters\n","input_size = 16 * 40\n","hidden_size = 256\n","output_size = 32 * 48\n","num_classes = 16\n","\n","model = DANNModel(num_classes)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"id":"fa0dfe31-c574-410f-a170-2bc69c20daa4","metadata":{"id":"fa0dfe31-c574-410f-a170-2bc69c20daa4"},"outputs":[],"source":["num_epochs = 200\n","alpha = 0.1\n","model = model\n","checkpoint_path = './Dann_checkpoint'\n","# Train the DANN model\n","train_dann(model, source_loader, target_loader, num_epochs, alpha, checkpoint_path)"]},{"cell_type":"code","execution_count":null,"id":"8baaea16-f132-4d5a-8b07-9b6a3c655b16","metadata":{"id":"8baaea16-f132-4d5a-8b07-9b6a3c655b16"},"outputs":[],"source":["y_te = np.squeeze(y_te)\n","ood_output = np.squeeze(ood_output)\n","y_pred_norm=model(torch.tensor(normalized_xte, dtype=torch.float32),alpha)[1]\n","y_pred_norm = y_pred_norm.detach().numpy()\n","reshaped_ypred = y_pred_norm.reshape(y_pred_norm.shape[0],-1)\n","y_pred1 = scaler2.inverse_transform(reshaped_ypred)\n","y_pred = y_pred1.reshape(y_pred_norm.shape)\n","y_te = y_te[:,np.newaxis,:,:]\n","print(np.mean(np.abs(y_pred-y_te)))\n","\n","ood_output = ood_output[:np.newaxis,:,:,]\n","y_pred_norm_ood=model(torch.tensor(normalized_xood, dtype=torch.float32),alpha)[1]\n","y_pred_norm_ood = y_pred_norm_ood.detach().numpy()\n","reshaped_ypred_ood = y_pred_norm_ood.reshape(y_pred_norm_ood.shape[0],-1)\n","y_pred_ood1 = scaler2.inverse_transform(reshaped_ypred_ood)\n","y_pred_ood = y_pred_ood1.reshape(ood_output.shape)\n","\n","print(np.mean(np.abs(y_pred_ood-ood_output)))"]},{"cell_type":"code","execution_count":null,"id":"5a8a1a95-49ab-44f9-8170-190259906bce","metadata":{"id":"5a8a1a95-49ab-44f9-8170-190259906bce"},"outputs":[],"source":["a = y_pred[5,0,:,:]\n","b = y_te[5,0,:,:]"]},{"cell_type":"code","execution_count":null,"id":"d70c7de6-fb5f-48bd-a2c9-0c2d3658606a","metadata":{"id":"d70c7de6-fb5f-48bd-a2c9-0c2d3658606a"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Create a sample matrix\n","matrix = a\n","\n","# Plot the colormap of the matrix\n","plt.imshow(matrix, cmap='viridis')\n","#plt.colorbar()\n","\n","# Show the plot\n","plt.show()\n","matrix = b\n","\n","# Plot the colormap of the matrix\n","plt.imshow(matrix, cmap='viridis')\n","#plt.colorbar()\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"3afdfdda-c532-4962-9163-9d528db6a014","metadata":{"id":"3afdfdda-c532-4962-9163-9d528db6a014"},"outputs":[],"source":["c= y_pred_ood[3,:,:]\n","d = ood_output[3,:,:]"]},{"cell_type":"code","execution_count":null,"id":"d474ad24-db96-40c6-bc5e-c9bbd7c59adc","metadata":{"id":"d474ad24-db96-40c6-bc5e-c9bbd7c59adc"},"outputs":[],"source":["matrix = c\n","\n","# Plot the colormap of the matrix\n","plt.imshow(matrix, cmap='viridis')\n","#plt.colorbar()\n","\n","# Show the plot\n","plt.show()\n","matrix = d\n","\n","# Plot the colormap of the matrix\n","plt.imshow(matrix, cmap='viridis')\n","#plt.colorbar()\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"61f25f8e-b23b-4b85-99b6-214e95ca40c7","metadata":{"id":"61f25f8e-b23b-4b85-99b6-214e95ca40c7"},"outputs":[],"source":["y_max = np.max(y_te.reshape(len(y_te),-1),axis = 1)\n","percent_err = np.mean(np.abs(y_pred-y_te)/y_max[:,np.newaxis,np.newaxis,np.newaxis])"]},{"cell_type":"code","execution_count":null,"id":"7ebe6142-f9df-47ad-9ab9-43b22f69caf4","metadata":{"id":"7ebe6142-f9df-47ad-9ab9-43b22f69caf4"},"outputs":[],"source":["percent_err"]},{"cell_type":"code","execution_count":null,"id":"cd88574c-5362-4be6-af9d-343cff54b669","metadata":{"id":"cd88574c-5362-4be6-af9d-343cff54b669"},"outputs":[],"source":["y_max = np.max(ood_output.reshape(len(ood_output),-1),axis = 1)\n","percent_err = np.mean(np.abs(y_pred_ood-ood_output)/y_max[:,np.newaxis,np.newaxis,np.newaxis])"]},{"cell_type":"code","execution_count":null,"id":"c1982811-24f2-40b3-a612-e29960dc5c64","metadata":{"id":"c1982811-24f2-40b3-a612-e29960dc5c64"},"outputs":[],"source":["percent_err"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}