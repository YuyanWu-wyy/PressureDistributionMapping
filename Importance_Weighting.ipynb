{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eae473-8650-4434-8373-55cb1e3a1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "datas = sio.loadmat('dataset_mapping.mat')\n",
    "bed_select=datas['bed_select']\n",
    "bed_whole=datas['bed_whole']\n",
    "pillow_up=datas['pillow_up']\n",
    "person = datas['person']\n",
    "posture = datas['posture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75660e3-ed3c-426d-bc65-ef15e2bf4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=15\n",
    "ood_idx = np.where(posture==i)[0]\n",
    "indis_idx = np.setdiff1d(np.arange(len(posture)), ood_idx)\n",
    "indis_input = bed_select[indis_idx,:,:]\n",
    "ood_input = bed_select[ood_idx,:,:]\n",
    "indis_output = pillow_up[indis_idx,:,:]\n",
    "ood_output = pillow_up[ood_idx,:,:]\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(indis_input, indis_output,train_size=0.9)\n",
    "scaler = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "reshaped_xtr = X_tr.reshape(X_tr.shape[0],-1)\n",
    "reshaped_xte = X_te.reshape(X_te.shape[0],-1)\n",
    "reshaped_ytr = y_tr.reshape(y_tr.shape[0],-1)\n",
    "reshaped_yte = y_te.reshape(y_te.shape[0],-1)\n",
    "reshaped_xood = ood_input.reshape(ood_input.shape[0],-1)\n",
    "reshaped_yood = ood_output.reshape(ood_output.shape[0],-1)\n",
    "\n",
    "normalized_xtr1 = scaler.fit_transform(reshaped_xtr)\n",
    "normalized_xte1 = scaler.transform(reshaped_xte)\n",
    "normalized_xood1 = scaler.transform(reshaped_xood)\n",
    "normalized_ytr1 = scaler2.fit_transform(reshaped_ytr)\n",
    "normalized_yte1 = scaler2.transform(reshaped_yte)\n",
    "normalized_yood1 = scaler2.transform(reshaped_yood)\n",
    "\n",
    "normalized_xtr = normalized_xtr1.reshape(X_tr.shape)\n",
    "normalized_xte = normalized_xte1.reshape(X_te.shape)\n",
    "normalized_ytr = normalized_ytr1.reshape(y_tr.shape)\n",
    "normalized_yte = normalized_yte1.reshape(y_te.shape)\n",
    "normalized_xood = normalized_xood1.reshape(ood_input.shape)\n",
    "normalized_yood = normalized_yood1.reshape(ood_output.shape)\n",
    "\n",
    "input_shape = (16,40,1)\n",
    "output_shape = (32,48,1)\n",
    "normalized_xtr = normalized_xtr[:,:,:,np.newaxis]\n",
    "normalized_xte = normalized_xte[:,:,:,np.newaxis]\n",
    "normalized_ytr = normalized_ytr[:,:,:,np.newaxis]\n",
    "normalized_yte = normalized_yte[:,:,:,np.newaxis]\n",
    "normalized_xood = normalized_xood[:,:,:,np.newaxis]\n",
    "normalized_yood = normalized_yood[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d28293-105a-4549-b32b-6c2f8d42c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from matplotlib import pyplot\n",
    "def create_model(input_shape=(16,40,1),output_shape=(32,48,1)):\n",
    "    init = RandomNormal(stddev=0.01)\n",
    "    in_image = Input(shape=input_shape)\n",
    "    n_filters1=32\n",
    "    g1 = Conv2D(n_filters1, (4,6), strides=(1,1), kernel_initializer=init)(in_image)\n",
    "    g1 = LeakyReLU(alpha=0.2)(g1)\n",
    "    g2= Conv2D(n_filters1*2, (3,5), strides=(1,1), kernel_initializer=init)(g1)\n",
    "    g2 = LeakyReLU(alpha=0.2)(g2)\n",
    "    g3= Conv2D(n_filters1*2, (3,5), strides=(1,1), kernel_initializer=init)(g2)\n",
    "    g3 = LeakyReLU(alpha=0.2)(g3)\n",
    "    g4= Conv2D(n_filters1*2, (2,4), strides=(1,1), kernel_initializer=init)(g3)\n",
    "    g4 = LeakyReLU(alpha=0.2)(g4)\n",
    "    g4= Conv2D(n_filters1*2, (2,4), strides=(1,1), kernel_initializer=init)(g4)\n",
    "    g4 = LeakyReLU(alpha=0.2)(g4)\n",
    "    g4 = Activation('relu')(g4)\n",
    "    g5=Conv2DTranspose(n_filters1*2, (2,4), strides=(1,1), kernel_initializer=init)(g4)\n",
    "    g5 = LeakyReLU(alpha=0.2)(g5)\n",
    "    g5=Conv2DTranspose(n_filters1*2, (7,7), strides=(1,1), kernel_initializer=init)(g5)\n",
    "    g5 = LeakyReLU(alpha=0.2)(g5)\n",
    "    g6=Conv2DTranspose(n_filters1*2, (7,7), strides=(1,1), kernel_initializer=init)(g5)\n",
    "    g6 = LeakyReLU(alpha=0.2)(g6)\n",
    "    g7=Conv2DTranspose(n_filters1, (7,7), strides=(1,1), kernel_initializer=init)(g6)\n",
    "    g7 = LeakyReLU(alpha=0.2)(g7)\n",
    "    g8=Conv2DTranspose(1, (7,7), strides=(1,1), kernel_initializer=init)(g7)\n",
    "    g8 = LeakyReLU(alpha=0.2)(g8)\n",
    "    out_image = Activation('relu')(g8)\n",
    "    model = Model(in_image, out_image)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afffe37-3e0a-4634-9ee1-62af61f120a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tensorflow.keras import losses\n",
    "normalized_xtr = np.squeeze(normalized_xtr)\n",
    "normalized_xte = np.squeeze(normalized_xte)\n",
    "normalized_ytr = np.squeeze(normalized_ytr)\n",
    "normalized_yte = np.squeeze(normalized_yte)\n",
    "normalized_xood = np.squeeze(normalized_xood)\n",
    "normalized_yood = np.squeeze(normalized_yood)\n",
    "y_te = np.squeeze(y_te)\n",
    "ood_output = np.squeeze(ood_output)\n",
    "sim_mat = np.zeros((normalized_xtr.shape[0],))\n",
    "for j in range(normalized_xtr.shape[0]):\n",
    "    tmp_mat = np.zeros((normalized_xood.shape[0],))\n",
    "    for k in range(normalized_xood.shape[0]):\n",
    "        s = ssim(normalized_xtr[j,:,:],normalized_xood[k,:,:], data_range=1)\n",
    "        tmp_mat[k] = s\n",
    "    sim_mat[j] = np.median(tmp_mat)\n",
    "input_shape = (16,40,1)\n",
    "output_shape = (32,48,1)\n",
    "normalized_xtr = normalized_xtr[:,:,:,np.newaxis]\n",
    "normalized_xte = normalized_xte[:,:,:,np.newaxis]\n",
    "normalized_ytr = normalized_ytr[:,:,:,np.newaxis]\n",
    "normalized_yte = normalized_yte[:,:,:,np.newaxis]\n",
    "normalized_xood = normalized_xood[:,:,:,np.newaxis]\n",
    "normalized_yood = normalized_yood[:,:,:,np.newaxis]\n",
    "model = create_model(input_shape, output_shape)\n",
    "lr = 0.1\n",
    "batch_size = 32\n",
    "train_epoch = 128\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "modelname = 'best_model_weight_' + str(i) + '.h5'\n",
    "checkpoint = ModelCheckpoint(modelname, monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "sample_weights = sim_mat[:,np.newaxis]\n",
    "history = model.fit(normalized_xtr,normalized_ytr,sample_weight = sample_weights,batch_size=batch_size, epochs=150,validation_data=(normalized_xte,normalized_yte), callbacks=[checkpoint])\n",
    "import keras\n",
    "model = keras.models.load_model(modelname)\n",
    "y_pred_norm=model.predict(normalized_xte)\n",
    "reshaped_ypred = y_pred_norm.reshape(y_pred_norm.shape[0],-1)\n",
    "y_pred1 = scaler2.inverse_transform(reshaped_ypred)\n",
    "y_pred = y_pred1.reshape(y_pred_norm.shape)\n",
    "y_te = y_te[:,:,:,np.newaxis]\n",
    "print(np.mean(np.abs(y_pred-y_te)))\n",
    "\n",
    "ood_output = ood_output[:,:,:,np.newaxis]\n",
    "y_pred_norm_ood=model.predict(normalized_xood)\n",
    "reshaped_ypred_ood = y_pred_norm_ood.reshape(y_pred_norm_ood.shape[0],-1)\n",
    "y_pred_ood1 = scaler2.inverse_transform(reshaped_ypred_ood)\n",
    "y_pred_ood = y_pred_ood1.reshape(ood_output.shape)\n",
    "\n",
    "print(np.mean(np.abs(y_pred_ood-ood_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73c7f6-1825-470b-89dc-e37595d12af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = y_pred[0,:,:,0]\n",
    "b = y_te[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc845b-b131-4d66-a0e6-5bded64c0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample matrix\n",
    "matrix = a\n",
    "\n",
    "# Plot the colormap of the matrix\n",
    "plt.imshow(matrix, cmap='viridis')\n",
    "#plt.colorbar()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "matrix = b\n",
    "\n",
    "# Plot the colormap of the matrix\n",
    "plt.imshow(matrix, cmap='viridis')\n",
    "#plt.colorbar()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4e9ab-67d4-4934-8213-c2e583c18e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c= y_pred_ood[3,:,:,0]\n",
    "d = ood_output[3,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b3cb6b-9aec-44fd-9d43-5872f10afc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = c\n",
    "\n",
    "# Plot the colormap of the matrix\n",
    "plt.imshow(matrix, cmap='viridis')\n",
    "#plt.colorbar()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "matrix = d\n",
    "\n",
    "# Plot the colormap of the matrix\n",
    "plt.imshow(matrix, cmap='viridis')\n",
    "#plt.colorbar()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56ac87-d017-4db4-92fe-ddc115fac51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max = np.max(y_te.reshape(len(y_te),-1),axis = 1)\n",
    "percent_err = np.mean(np.abs(y_pred-y_te)/y_max[:,np.newaxis,np.newaxis,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79492e1-ba43-41b6-86ac-31922c58bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671f56f-1e77-4ce3-ba1c-627e713489e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max = np.max(ood_output.reshape(len(ood_output),-1),axis = 1)\n",
    "percent_err = np.mean(np.abs(y_pred_ood-ood_output)/y_max[:,np.newaxis,np.newaxis,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14e142-27ed-43a8-b9bf-a29e17a57607",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_err"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
